{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Job 2: Aggregations and GroupBy Operations\n\n\nLearning Objectives:\n- GroupBy and aggregate functions (sum, avg, count, min, max)\n- Multiple aggregations on the same DataFrame\n- Window functions for running totals and rankings\n- Partitioning and ordering data\n\nKey Concepts:\n- GroupBy creates a GroupedData object (not a DataFrame)\n- Aggregations are wide transformations (cause shuffle)\n- Window functions allow row-level operations with group context"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Import Libraries and Initialize Spark"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add project root to path  \nproject_root = os.path.dirname(os.getcwd())\nsys.path.append(project_root)\n\nimport sys\nimport os\n\nprint(\"\u2705 Libraries imported successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Spark Session\nspark = get_spark_session(\"Job 2: Aggregations and GroupBy Operations\")\ndata_dir = get_data_dir()\n\nprint(f\"\u2705 Spark session created!\")\nprint(f\"\ud83d\udcca Spark UI: http://localhost:4040\")\nprint(f\"\ud83d\udcc1 Data directory: {data_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate sample data if it doesn't exist\nfrom utils.data_generator import generate_all_datasets\nimport os\n\ndata_files = os.path.join(data_dir, \"users.csv\")\nif not os.path.exists(data_files):\n    print(\"\ud83d\udcc1 Sample data not found. Generating...\")\n    generate_all_datasets(data_dir)\n    print(\"\u2705 Sample data generated!\")\nelse:\n    print(\"\u2705 Sample data already exists.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 1: Simple aggregations on entire DataFrame\n\n    \n    Aggregate functions compute a single value from multiple rows.\n    Without groupBy, they work on the entire DataFrame."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 1: Simple Aggregations (Entire DataFrame)\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Count all rows:\")\n    total_count = df.count()\n    print(f\"Total transactions: {total_count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Sum, Average, Min, Max on entire DataFrame:\")\n    df.select(\n        spark_sum(\"amount\").alias(\"total_revenue\"),\n        avg(\"amount\").alias(\"avg_amount\"),\n        spark_min(\"amount\").alias(\"min_amount\"),\n        spark_max(\"amount\").alias(\"max_amount\"),\n        count(\"*\").alias(\"row_count\")\n    ).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Count distinct values:\")\n    df.select(\n        countDistinct(\"user_id\").alias(\"unique_users\"),\n        countDistinct(\"product_id\").alias(\"unique_products\"),\n        countDistinct(\"status\").alias(\"unique_statuses\")\n    ).show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 2: GroupBy with aggregations\n\n    \n    GroupBy splits data into groups and applies aggregations to each group.\n    This is one of the most common operations in data analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 2: GroupBy Aggregations\")\n    print(\"=\"*70)\n    \n    # Group by single column"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Revenue by status:\")\n    df.groupBy(\"status\") \\\n        .agg(\n            count(\"*\").alias(\"transaction_count\"),\n            spark_sum(\"amount\").alias(\"total_revenue\"),\n            avg(\"amount\").alias(\"avg_amount\")\n        ) \\\n        .orderBy(desc(\"total_revenue\")) \\\n        .show()\n    \n    # Group by multiple columns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Transactions by status and date:\")\n    df.groupBy(\"status\", \"transaction_date\") \\\n        .agg(\n            count(\"*\").alias(\"count\"),\n            spark_sum(\"amount\").alias(\"total_amount\")\n        ) \\\n        .orderBy(\"transaction_date\", \"status\") \\\n        .show(10)\n    \n    # Multiple aggregations at once"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Comprehensive user statistics:\")\n    user_stats = df.groupBy(\"user_id\") \\\n        .agg(\n            count(\"*\").alias(\"num_transactions\"),\n            spark_sum(\"amount\").alias(\"total_spent\"),\n            avg(\"amount\").alias(\"avg_transaction\"),\n            spark_min(\"amount\").alias(\"min_transaction\"),\n            spark_max(\"amount\").alias(\"max_transaction\"),\n            countDistinct(\"product_id\").alias(\"unique_products\")\n        ) \\\n        .orderBy(desc(\"total_spent\"))\n    \n    print(\"Top 10 users by total spent:\")\n    user_stats.show(10)\n    \n    # Filtering after aggregation (having clause)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Users with more than 10 transactions:\")\n    user_stats.filter(col(\"num_transactions\") > 10).show(10)\n    \n    # Collecting values into lists"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Products bought by each user (collect_list):\")\n    df.groupBy(\"user_id\") \\\n        .agg(\n            collect_set(\"product_id\").alias(\"products_bought\")\n        ) \\\n        .show(5, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 3: Advanced GroupBy patterns\n\n    \n    More sophisticated aggregation patterns you'll use in real-world scenarios."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 3: Advanced GroupBy Patterns\")\n    print(\"=\"*70)\n    \n    # Conditional aggregation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Conditional aggregation (count by condition):\")\n    df.groupBy(\"user_id\") \\\n        .agg(\n            count(\"*\").alias(\"total_transactions\"),\n            spark_sum(when(col(\"status\") == \"completed\", 1).otherwise(0)).alias(\"completed\"),\n            spark_sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)).alias(\"cancelled\"),\n            spark_sum(when(col(\"status\") == \"completed\", col(\"amount\")).otherwise(0)).alias(\"completed_revenue\")\n        ) \\\n        .orderBy(desc(\"total_transactions\")) \\\n        .show(10)\n    \n    # Aggregation with percentage calculation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Revenue distribution by status (with percentages):\")\n    status_revenue = df.groupBy(\"status\") \\\n        .agg(\n            count(\"*\").alias(\"count\"),\n            spark_sum(\"amount\").alias(\"revenue\")\n        )\n    \n    total_revenue = df.select(spark_sum(\"amount\")).first()[0]\n    \n    status_revenue.withColumn(\n        \"percentage\",\n        spark_round((col(\"revenue\") / total_revenue) * 100, 2)\n    ).orderBy(desc(\"revenue\")).show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 4: Introduction to Window Functions\n\n    \n    Window functions perform calculations across a set of rows that are\n    related to the current row. Unlike groupBy, they don't collapse rows.\n    \n    Key concepts:\n    - Window specification: defines the \"window\" of rows\n    - Partition by: group rows (like groupBy)\n    - Order by: order within each partition\n    - Frame: which rows in the partition to include"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 4: Window Functions - Introduction\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Running total per user:\")\n    # Define window: partition by user, order by date\n    window_spec = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n    \n    df_with_running_total = df \\\n        .withColumn(\"running_total\", spark_sum(\"amount\").over(window_spec)) \\\n        .withColumn(\"transaction_number\", row_number().over(window_spec))\n    \n    # Show running total for a few users\n    df_with_running_total \\\n        .filter(col(\"user_id\").isin([1, 2, 3])) \\\n        .select(\"user_id\", \"transaction_date\", \"amount\", \"transaction_number\", \"running_total\") \\\n        .orderBy(\"user_id\", \"transaction_date\") \\\n        .show(20)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Average amount per user (window avg):\")\n    # Calculate average transaction amount per user\n    user_window = Window.partitionBy(\"user_id\")\n    \n    df.withColumn(\"user_avg_amount\", avg(\"amount\").over(user_window)) \\\n        .withColumn(\"diff_from_avg\", col(\"amount\") - col(\"user_avg_amount\")) \\\n        .select(\"user_id\", \"amount\", \"user_avg_amount\", \"diff_from_avg\") \\\n        .show(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 5: Window Functions - Ranking\n\n    \n    Ranking functions assign ranks to rows within partitions.\n    \n    - row_number(): Sequential number (1, 2, 3, 4...)\n    - rank(): Rank with gaps (1, 2, 2, 4...)\n    - dense_rank(): Rank without gaps (1, 2, 2, 3...)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 5: Window Functions - Ranking\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Top 3 transactions per user:\")\n    # Rank transactions by amount within each user\n    user_amount_window = Window.partitionBy(\"user_id\").orderBy(desc(\"amount\"))\n    \n    df.withColumn(\"rank\", row_number().over(user_amount_window)) \\\n        .filter(col(\"rank\") <= 3) \\\n        .select(\"user_id\", \"amount\", \"transaction_date\", \"rank\") \\\n        .orderBy(\"user_id\", \"rank\") \\\n        .show(20)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Difference between rank() and dense_rank():\")\n    # Create sample data with ties\n    sample_data = [\n        (1, 100),\n        (2, 100),\n        (3, 90),\n        (4, 90),\n        (5, 80),\n    ]\n    sample_df = spark.createDataFrame(sample_data, [\"id\", \"score\"])\n    \n    window = Window.orderBy(desc(\"score\"))\n    \n    sample_df.select(\n        col(\"id\"),\n        col(\"score\"),\n        row_number().over(window).alias(\"row_number\"),\n        rank().over(window).alias(\"rank\"),\n        dense_rank().over(window).alias(\"dense_rank\")\n    ).show()\n    \n    print(\"  row_number: Always sequential (1,2,3,4,5)\")\n    print(\"  rank:       Has gaps after ties (1,1,3,3,5)\")\n    print(\"  dense_rank: No gaps (1,1,2,2,3)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 6: Window Functions - Lag and Lead\n\n    \n    lag() and lead() access data from previous/next rows.\n    Useful for comparing values across rows or calculating deltas."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 6: Window Functions - Lag and Lead\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Compare current transaction with previous one:\")\n    # Window ordered by date for each user\n    user_date_window = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n    \n    df_with_prev = df \\\n        .withColumn(\"prev_amount\", lag(\"amount\", 1).over(user_date_window)) \\\n        .withColumn(\"next_amount\", lead(\"amount\", 1).over(user_date_window)) \\\n        .withColumn(\"amount_change\", col(\"amount\") - col(\"prev_amount\"))\n    \n    df_with_prev \\\n        .filter(col(\"user_id\").isin([1, 2])) \\\n        .select(\"user_id\", \"transaction_date\", \"prev_amount\", \"amount\", \"next_amount\", \"amount_change\") \\\n        .orderBy(\"user_id\", \"transaction_date\") \\\n        .show(15)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 First and last transaction per user:\")\n    df.withColumn(\"first_amount\", first(\"amount\").over(user_date_window)) \\\n        .withColumn(\"last_amount\", last(\"amount\").over(user_date_window)) \\\n        .select(\"user_id\", \"amount\", \"first_amount\", \"last_amount\") \\\n        .distinct() \\\n        .show(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 7: Practical example - Product analytics\n\n    \n    Combining what we've learned to solve a real analytics problem."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 7: Practical Example - Product Performance Analysis\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Top performing products:\")\n    \n    # Product-level metrics\n    product_metrics = transactions_df \\\n        .filter(col(\"status\") == \"completed\") \\\n        .groupBy(\"product_id\") \\\n        .agg(\n            count(\"*\").alias(\"num_sales\"),\n            spark_sum(\"amount\").alias(\"total_revenue\"),\n            avg(\"amount\").alias(\"avg_sale_price\"),\n            countDistinct(\"user_id\").alias(\"unique_buyers\")\n        )\n    \n    # Join with products to get product names\n    product_performance = product_metrics \\\n        .join(products_df, \"product_id\") \\\n        .select(\n            \"product_id\",\n            \"name\",\n            \"category\",\n            \"num_sales\",\n            \"total_revenue\",\n            \"avg_sale_price\",\n            \"unique_buyers\"\n        ) \\\n        .orderBy(desc(\"total_revenue\"))\n    \n    print(\"Overall product performance:\")\n    product_performance.show(10, truncate=False)\n    \n    # Rank products within each category"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Top 3 products per category:\")\n    category_window = Window.partitionBy(\"category\").orderBy(desc(\"total_revenue\"))\n    \n    product_performance \\\n        .withColumn(\"rank_in_category\", row_number().over(category_window)) \\\n        .filter(col(\"rank_in_category\") <= 3) \\\n        .select(\"category\", \"name\", \"total_revenue\", \"num_sales\", \"rank_in_category\") \\\n        .orderBy(\"category\", \"rank_in_category\") \\\n        .show(20, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n\u2705 Notebook completed! Check the next notebook to continue learning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cleanup (Optional)\n\nUncomment and run to stop Spark session:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# stop_spark_session(spark)\n# print(\"\u2705 Spark session stopped.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}