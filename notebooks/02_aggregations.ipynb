{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job 2: Aggregations and GroupBy Operations\n",
    "\n",
    "\n",
    "Learning Objectives:\n",
    "- GroupBy and aggregate functions (sum, avg, count, min, max)\n",
    "- Multiple aggregations on the same DataFrame\n",
    "- Window functions for running totals and rankings\n",
    "- Partitioning and ordering data\n",
    "\n",
    "Key Concepts:\n",
    "- GroupBy creates a GroupedData object (not a DataFrame)\n",
    "- Aggregations are wide transformations (cause shuffle)\n",
    "- Window functions allow row-level operations with group context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set JAVA_HOME before any PySpark imports\n",
    "java_home = '/Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home'\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path, override=False)\n",
    "\n",
    "# Add project root to path  \n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from utils.spark_session import get_spark_session, stop_spark_session, get_data_dir\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Job 2: Aggregations and GroupBy Operations\")\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "print(f\"âœ… Spark session created!\")\n",
    "print(f\"ðŸ“Š Spark UI: http://localhost:4040\")\n",
    "print(f\"ðŸ“ Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "from utils.data_generator import generate_all_datasets\n",
    "import os\n",
    "\n",
    "data_files = os.path.join(data_dir, \"users.csv\")\n",
    "if not os.path.exists(data_files):\n",
    "    print(\"ðŸ“ Sample data not found. Generating...\")\n",
    "    generate_all_datasets(data_dir)\n",
    "    print(\"âœ… Sample data generated!\")\n",
    "else:\n",
    "    print(\"âœ… Sample data already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 1: Simple aggregations on entire DataFrame\n",
    "\n",
    "    \n",
    "    Aggregate functions compute a single value from multiple rows.\n",
    "    Without groupBy, they work on the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 1: Simple Aggregations (Entire DataFrame)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Count all rows:\")\n",
    "    total_count = df.count()\n",
    "    print(f\"Total transactions: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Sum, Average, Min, Max on entire DataFrame:\")\n",
    "    df.select(\n",
    "        spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\"),\n",
    "        spark_min(\"amount\").alias(\"min_amount\"),\n",
    "        spark_max(\"amount\").alias(\"max_amount\"),\n",
    "        count(\"*\").alias(\"row_count\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Count distinct values:\")\n",
    "    df.select(\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "        countDistinct(\"status\").alias(\"unique_statuses\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 2: GroupBy with aggregations\n",
    "\n",
    "    \n",
    "    GroupBy splits data into groups and applies aggregations to each group.\n",
    "    This is one of the most common operations in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 2: GroupBy Aggregations\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Group by single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Revenue by status:\")\n",
    "    df.groupBy(\"status\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"transaction_count\"),\n",
    "            spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_amount\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_revenue\")) \\\n",
    "        .show()\n",
    "    \n",
    "    # Group by multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Transactions by status and date:\")\n",
    "    df.groupBy(\"status\", \"transaction_date\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            spark_sum(\"amount\").alias(\"total_amount\")\n",
    "        ) \\\n",
    "        .orderBy(\"transaction_date\", \"status\") \\\n",
    "        .show(10)\n",
    "    \n",
    "    # Multiple aggregations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Comprehensive user statistics:\")\n",
    "    user_stats = df.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"num_transactions\"),\n",
    "            spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "            avg(\"amount\").alias(\"avg_transaction\"),\n",
    "            spark_min(\"amount\").alias(\"min_transaction\"),\n",
    "            spark_max(\"amount\").alias(\"max_transaction\"),\n",
    "            countDistinct(\"product_id\").alias(\"unique_products\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_spent\"))\n",
    "    \n",
    "    print(\"Top 10 users by total spent:\")\n",
    "    user_stats.show(10)\n",
    "    \n",
    "    # Filtering after aggregation (having clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Users with more than 10 transactions:\")\n",
    "    user_stats.filter(col(\"num_transactions\") > 10).show(10)\n",
    "    \n",
    "    # Collecting values into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Products bought by each user (collect_list):\")\n",
    "    df.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            collect_set(\"product_id\").alias(\"products_bought\")\n",
    "        ) \\\n",
    "        .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 3: Advanced GroupBy patterns\n",
    "\n",
    "    \n",
    "    More sophisticated aggregation patterns you'll use in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 3: Advanced GroupBy Patterns\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Conditional aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Conditional aggregation (count by condition):\")\n",
    "    df.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_transactions\"),\n",
    "            spark_sum(when(col(\"status\") == \"completed\", 1).otherwise(0)).alias(\"completed\"),\n",
    "            spark_sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)).alias(\"cancelled\"),\n",
    "            spark_sum(when(col(\"status\") == \"completed\", col(\"amount\")).otherwise(0)).alias(\"completed_revenue\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_transactions\")) \\\n",
    "        .show(10)\n",
    "    \n",
    "    # Aggregation with percentage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Revenue distribution by status (with percentages):\")\n",
    "    status_revenue = df.groupBy(\"status\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            spark_sum(\"amount\").alias(\"revenue\")\n",
    "        )\n",
    "    \n",
    "    total_revenue = df.select(spark_sum(\"amount\")).first()[0]\n",
    "    \n",
    "    status_revenue.withColumn(\n",
    "        \"percentage\",\n",
    "        spark_round((col(\"revenue\") / total_revenue) * 100, 2)\n",
    "    ).orderBy(desc(\"revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 4: Introduction to Window Functions\n",
    "\n",
    "    \n",
    "    Window functions perform calculations across a set of rows that are\n",
    "    related to the current row. Unlike groupBy, they don't collapse rows.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Window specification: defines the \"window\" of rows\n",
    "    - Partition by: group rows (like groupBy)\n",
    "    - Order by: order within each partition\n",
    "    - Frame: which rows in the partition to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 4: Window Functions - Introduction\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Running total per user:\")\n",
    "    # Define window: partition by user, order by date\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n",
    "    \n",
    "    df_with_running_total = df \\\n",
    "        .withColumn(\"running_total\", spark_sum(\"amount\").over(window_spec)) \\\n",
    "        .withColumn(\"transaction_number\", row_number().over(window_spec))\n",
    "    \n",
    "    # Show running total for a few users\n",
    "    df_with_running_total \\\n",
    "        .filter(col(\"user_id\").isin([1, 2, 3])) \\\n",
    "        .select(\"user_id\", \"transaction_date\", \"amount\", \"transaction_number\", \"running_total\") \\\n",
    "        .orderBy(\"user_id\", \"transaction_date\") \\\n",
    "        .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Average amount per user (window avg):\")\n",
    "    # Calculate average transaction amount per user\n",
    "    user_window = Window.partitionBy(\"user_id\")\n",
    "    \n",
    "    df.withColumn(\"user_avg_amount\", avg(\"amount\").over(user_window)) \\\n",
    "        .withColumn(\"diff_from_avg\", col(\"amount\") - col(\"user_avg_amount\")) \\\n",
    "        .select(\"user_id\", \"amount\", \"user_avg_amount\", \"diff_from_avg\") \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 5: Window Functions - Ranking\n",
    "\n",
    "    \n",
    "    Ranking functions assign ranks to rows within partitions.\n",
    "    \n",
    "    - row_number(): Sequential number (1, 2, 3, 4...)\n",
    "    - rank(): Rank with gaps (1, 2, 2, 4...)\n",
    "    - dense_rank(): Rank without gaps (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 5: Window Functions - Ranking\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top 3 transactions per user:\")\n",
    "    # Rank transactions by amount within each user\n",
    "    user_amount_window = Window.partitionBy(\"user_id\").orderBy(desc(\"amount\"))\n",
    "    \n",
    "    df.withColumn(\"rank\", row_number().over(user_amount_window)) \\\n",
    "        .filter(col(\"rank\") <= 3) \\\n",
    "        .select(\"user_id\", \"amount\", \"transaction_date\", \"rank\") \\\n",
    "        .orderBy(\"user_id\", \"rank\") \\\n",
    "        .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Difference between rank() and dense_rank():\")\n",
    "    # Create sample data with ties\n",
    "    sample_data = [\n",
    "        (1, 100),\n",
    "        (2, 100),\n",
    "        (3, 90),\n",
    "        (4, 90),\n",
    "        (5, 80),\n",
    "    ]\n",
    "    sample_df = spark.createDataFrame(sample_data, [\"id\", \"score\"])\n",
    "    \n",
    "    window = Window.orderBy(desc(\"score\"))\n",
    "    \n",
    "    sample_df.select(\n",
    "        col(\"id\"),\n",
    "        col(\"score\"),\n",
    "        row_number().over(window).alias(\"row_number\"),\n",
    "        rank().over(window).alias(\"rank\"),\n",
    "        dense_rank().over(window).alias(\"dense_rank\")\n",
    "    ).show()\n",
    "    \n",
    "    print(\"  row_number: Always sequential (1,2,3,4,5)\")\n",
    "    print(\"  rank:       Has gaps after ties (1,1,3,3,5)\")\n",
    "    print(\"  dense_rank: No gaps (1,1,2,2,3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 6: Window Functions - Lag and Lead\n",
    "\n",
    "    \n",
    "    lag() and lead() access data from previous/next rows.\n",
    "    Useful for comparing values across rows or calculating deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 6: Window Functions - Lag and Lead\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Compare current transaction with previous one:\")\n",
    "    # Window ordered by date for each user\n",
    "    user_date_window = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n",
    "    \n",
    "    df_with_prev = df \\\n",
    "        .withColumn(\"prev_amount\", lag(\"amount\", 1).over(user_date_window)) \\\n",
    "        .withColumn(\"next_amount\", lead(\"amount\", 1).over(user_date_window)) \\\n",
    "        .withColumn(\"amount_change\", col(\"amount\") - col(\"prev_amount\"))\n",
    "    \n",
    "    df_with_prev \\\n",
    "        .filter(col(\"user_id\").isin([1, 2])) \\\n",
    "        .select(\"user_id\", \"transaction_date\", \"prev_amount\", \"amount\", \"next_amount\", \"amount_change\") \\\n",
    "        .orderBy(\"user_id\", \"transaction_date\") \\\n",
    "        .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ First and last transaction per user:\")\n",
    "    df.withColumn(\"first_amount\", first(\"amount\").over(user_date_window)) \\\n",
    "        .withColumn(\"last_amount\", last(\"amount\").over(user_date_window)) \\\n",
    "        .select(\"user_id\", \"amount\", \"first_amount\", \"last_amount\") \\\n",
    "        .distinct() \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 7: Practical example - Product analytics\n",
    "\n",
    "    \n",
    "    Combining what we've learned to solve a real analytics problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 7: Practical Example - Product Performance Analysis\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top performing products:\")\n",
    "    \n",
    "    # Product-level metrics\n",
    "    product_metrics = transactions_df \\\n",
    "        .filter(col(\"status\") == \"completed\") \\\n",
    "        .groupBy(\"product_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"num_sales\"),\n",
    "            spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_sale_price\"),\n",
    "            countDistinct(\"user_id\").alias(\"unique_buyers\")\n",
    "        )\n",
    "    \n",
    "    # Join with products to get product names\n",
    "    product_performance = product_metrics \\\n",
    "        .join(products_df, \"product_id\") \\\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"name\",\n",
    "            \"category\",\n",
    "            \"num_sales\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_sale_price\",\n",
    "            \"unique_buyers\"\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    print(\"Overall product performance:\")\n",
    "    product_performance.show(10, truncate=False)\n",
    "    \n",
    "    # Rank products within each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top 3 products per category:\")\n",
    "    category_window = Window.partitionBy(\"category\").orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    product_performance \\\n",
    "        .withColumn(\"rank_in_category\", row_number().over(category_window)) \\\n",
    "        .filter(col(\"rank_in_category\") <= 3) \\\n",
    "        .select(\"category\", \"name\", \"total_revenue\", \"num_sales\", \"rank_in_category\") \\\n",
    "        .orderBy(\"category\", \"rank_in_category\") \\\n",
    "        .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Notebook completed! Check the next notebook to continue learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run to stop Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_spark_session(spark)\n",
    "# print(\"âœ… Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-pyspark-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
