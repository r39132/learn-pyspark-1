{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job 2: Aggregations and GroupBy Operations\n",
    "\n",
    "\n",
    "Learning Objectives:\n",
    "- GroupBy and aggregate functions (sum, avg, count, min, max)\n",
    "- Multiple aggregations on the same DataFrame\n",
    "- Window functions for running totals and rankings\n",
    "- Partitioning and ordering data\n",
    "\n",
    "Key Concepts:\n",
    "- GroupBy creates a GroupedData object (not a DataFrame)\n",
    "- Aggregations are wide transformations (cause shuffle)\n",
    "- Window functions allow row-level operations with group context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set JAVA_HOME before any PySpark imports\n",
    "java_home = '/Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home'\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path, override=False)\n",
    "\n",
    "# Add project root to path  \n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Note: Log4j2 configuration is in log4j2.properties and applied by utils/spark_session.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from utils.spark_session import get_spark_session, stop_spark_session, get_data_dir\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m spark = \u001b[43mget_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mJob 2: Aggregations and GroupBy Operations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m data_dir = get_data_dir()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Spark session created!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/utils/spark_session.py:40\u001b[39m, in \u001b[36mget_spark_session\u001b[39m\u001b[34m(app_name, local_mode)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Suppress excessive logging (set to INFO, WARN, or ERROR)\u001b[39;00m\n\u001b[32m     37\u001b[39m builder = builder.config(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.extraJavaOptions\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     38\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33m-Dlog4j.configuration=file:log4j.properties\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m spark = \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Set log level to WARN to reduce console output\u001b[39;00m\n\u001b[32m     43\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py:97\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m         signal.signal(signal.SIGINT, signal.SIG_IGN)\n\u001b[32m     96\u001b[39m     popen_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpreexec_fn\u001b[39m\u001b[33m\"\u001b[39m] = preexec_func\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     proc = \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[32m    100\u001b[39m     proc = Popen(command, **popen_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py:1950\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1948\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errno_num != \u001b[32m0\u001b[39m:\n\u001b[32m   1949\u001b[39m         err_msg = os.strerror(errno_num)\n\u001b[32m-> \u001b[39m\u001b[32m1950\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1951\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Job 2: Aggregations and GroupBy Operations\")\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "print(f\"âœ… Spark session created!\")\n",
    "print(f\"ðŸ“Š Spark UI: http://localhost:4040\")\n",
    "print(f\"ðŸ“ Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_all_datasets\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data_files = os.path.join(\u001b[43mdata_dir\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33musers.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(data_files):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“ Sample data not found. Generating...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "from utils.data_generator import generate_all_datasets\n",
    "import os\n",
    "\n",
    "data_files = os.path.join(data_dir, \"users.csv\")\n",
    "if not os.path.exists(data_files):\n",
    "    print(\"ðŸ“ Sample data not found. Generating...\")\n",
    "    generate_all_datasets(data_dir)\n",
    "    print(\"âœ… Sample data generated!\")\n",
    "else:\n",
    "    print(\"âœ… Sample data already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 1: Simple aggregations on entire DataFrame\n",
    "\n",
    "    \n",
    "    Aggregate functions compute a single value from multiple rows.\n",
    "    Without groupBy, they work on the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (192665221.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"LESSON 1: Simple Aggregations (Entire DataFrame)\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 1: Simple Aggregations (Entire DataFrame)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Count all rows:\")\n",
    "    total_count = df.count()\n",
    "    print(f\"Total transactions: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1095160598.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf.select(\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”¹ Sum, Average, Min, Max on entire DataFrame:\")\n",
    "    df.select(\n",
    "        spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\"),\n",
    "        spark_min(\"amount\").alias(\"min_amount\"),\n",
    "        spark_max(\"amount\").alias(\"max_amount\"),\n",
    "        count(\"*\").alias(\"row_count\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Count distinct values:\")\n",
    "    df.select(\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "        countDistinct(\"status\").alias(\"unique_statuses\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 2: GroupBy with aggregations\n",
    "\n",
    "    \n",
    "    GroupBy splits data into groups and applies aggregations to each group.\n",
    "    This is one of the most common operations in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 2: GroupBy Aggregations\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Group by single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Revenue by status:\")\n",
    "    df.groupBy(\"status\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"transaction_count\"),\n",
    "            spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_amount\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_revenue\")) \\\n",
    "        .show()\n",
    "    \n",
    "    # Group by multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Transactions by status and date:\")\n",
    "    df.groupBy(\"status\", \"transaction_date\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            spark_sum(\"amount\").alias(\"total_amount\")\n",
    "        ) \\\n",
    "        .orderBy(\"transaction_date\", \"status\") \\\n",
    "        .show(10)\n",
    "    \n",
    "    # Multiple aggregations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Comprehensive user statistics:\")\n",
    "    user_stats = df.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"num_transactions\"),\n",
    "            spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "            avg(\"amount\").alias(\"avg_transaction\"),\n",
    "            spark_min(\"amount\").alias(\"min_transaction\"),\n",
    "            spark_max(\"amount\").alias(\"max_transaction\"),\n",
    "            countDistinct(\"product_id\").alias(\"unique_products\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_spent\"))\n",
    "    \n",
    "    print(\"Top 10 users by total spent:\")\n",
    "    user_stats.show(10)\n",
    "    \n",
    "    # Filtering after aggregation (having clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Users with more than 10 transactions:\")\n",
    "    user_stats.filter(col(\"num_transactions\") > 10).show(10)\n",
    "    \n",
    "    # Collecting values into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Products bought by each user (collect_list):\")\n",
    "    df.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            collect_set(\"product_id\").alias(\"products_bought\")\n",
    "        ) \\\n",
    "        .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 3: Advanced GroupBy patterns\n",
    "\n",
    "    \n",
    "    More sophisticated aggregation patterns you'll use in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 3: Advanced GroupBy Patterns\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Conditional aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Conditional aggregation (count by condition):\")\n",
    "    df.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_transactions\"),\n",
    "            spark_sum(when(col(\"status\") == \"completed\", 1).otherwise(0)).alias(\"completed\"),\n",
    "            spark_sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)).alias(\"cancelled\"),\n",
    "            spark_sum(when(col(\"status\") == \"completed\", col(\"amount\")).otherwise(0)).alias(\"completed_revenue\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_transactions\")) \\\n",
    "        .show(10)\n",
    "    \n",
    "    # Aggregation with percentage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Revenue distribution by status (with percentages):\")\n",
    "    status_revenue = df.groupBy(\"status\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            spark_sum(\"amount\").alias(\"revenue\")\n",
    "        )\n",
    "    \n",
    "    total_revenue = df.select(spark_sum(\"amount\")).first()[0]\n",
    "    \n",
    "    status_revenue.withColumn(\n",
    "        \"percentage\",\n",
    "        spark_round((col(\"revenue\") / total_revenue) * 100, 2)\n",
    "    ).orderBy(desc(\"revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 4: Introduction to Window Functions\n",
    "\n",
    "    \n",
    "    Window functions perform calculations across a set of rows that are\n",
    "    related to the current row. Unlike groupBy, they don't collapse rows.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Window specification: defines the \"window\" of rows\n",
    "    - Partition by: group rows (like groupBy)\n",
    "    - Order by: order within each partition\n",
    "    - Frame: which rows in the partition to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 4: Window Functions - Introduction\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Running total per user:\")\n",
    "    # Define window: partition by user, order by date\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n",
    "    \n",
    "    df_with_running_total = df \\\n",
    "        .withColumn(\"running_total\", spark_sum(\"amount\").over(window_spec)) \\\n",
    "        .withColumn(\"transaction_number\", row_number().over(window_spec))\n",
    "    \n",
    "    # Show running total for a few users\n",
    "    df_with_running_total \\\n",
    "        .filter(col(\"user_id\").isin([1, 2, 3])) \\\n",
    "        .select(\"user_id\", \"transaction_date\", \"amount\", \"transaction_number\", \"running_total\") \\\n",
    "        .orderBy(\"user_id\", \"transaction_date\") \\\n",
    "        .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Average amount per user (window avg):\")\n",
    "    # Calculate average transaction amount per user\n",
    "    user_window = Window.partitionBy(\"user_id\")\n",
    "    \n",
    "    df.withColumn(\"user_avg_amount\", avg(\"amount\").over(user_window)) \\\n",
    "        .withColumn(\"diff_from_avg\", col(\"amount\") - col(\"user_avg_amount\")) \\\n",
    "        .select(\"user_id\", \"amount\", \"user_avg_amount\", \"diff_from_avg\") \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 5: Window Functions - Ranking\n",
    "\n",
    "    \n",
    "    Ranking functions assign ranks to rows within partitions.\n",
    "    \n",
    "    - row_number(): Sequential number (1, 2, 3, 4...)\n",
    "    - rank(): Rank with gaps (1, 2, 2, 4...)\n",
    "    - dense_rank(): Rank without gaps (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 5: Window Functions - Ranking\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top 3 transactions per user:\")\n",
    "    # Rank transactions by amount within each user\n",
    "    user_amount_window = Window.partitionBy(\"user_id\").orderBy(desc(\"amount\"))\n",
    "    \n",
    "    df.withColumn(\"rank\", row_number().over(user_amount_window)) \\\n",
    "        .filter(col(\"rank\") <= 3) \\\n",
    "        .select(\"user_id\", \"amount\", \"transaction_date\", \"rank\") \\\n",
    "        .orderBy(\"user_id\", \"rank\") \\\n",
    "        .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Difference between rank() and dense_rank():\")\n",
    "    # Create sample data with ties\n",
    "    sample_data = [\n",
    "        (1, 100),\n",
    "        (2, 100),\n",
    "        (3, 90),\n",
    "        (4, 90),\n",
    "        (5, 80),\n",
    "    ]\n",
    "    sample_df = spark.createDataFrame(sample_data, [\"id\", \"score\"])\n",
    "    \n",
    "    window = Window.orderBy(desc(\"score\"))\n",
    "    \n",
    "    sample_df.select(\n",
    "        col(\"id\"),\n",
    "        col(\"score\"),\n",
    "        row_number().over(window).alias(\"row_number\"),\n",
    "        rank().over(window).alias(\"rank\"),\n",
    "        dense_rank().over(window).alias(\"dense_rank\")\n",
    "    ).show()\n",
    "    \n",
    "    print(\"  row_number: Always sequential (1,2,3,4,5)\")\n",
    "    print(\"  rank:       Has gaps after ties (1,1,3,3,5)\")\n",
    "    print(\"  dense_rank: No gaps (1,1,2,2,3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 6: Window Functions - Lag and Lead\n",
    "\n",
    "    \n",
    "    lag() and lead() access data from previous/next rows.\n",
    "    Useful for comparing values across rows or calculating deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 6: Window Functions - Lag and Lead\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Compare current transaction with previous one:\")\n",
    "    # Window ordered by date for each user\n",
    "    user_date_window = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\")\n",
    "    \n",
    "    df_with_prev = df \\\n",
    "        .withColumn(\"prev_amount\", lag(\"amount\", 1).over(user_date_window)) \\\n",
    "        .withColumn(\"next_amount\", lead(\"amount\", 1).over(user_date_window)) \\\n",
    "        .withColumn(\"amount_change\", col(\"amount\") - col(\"prev_amount\"))\n",
    "    \n",
    "    df_with_prev \\\n",
    "        .filter(col(\"user_id\").isin([1, 2])) \\\n",
    "        .select(\"user_id\", \"transaction_date\", \"prev_amount\", \"amount\", \"next_amount\", \"amount_change\") \\\n",
    "        .orderBy(\"user_id\", \"transaction_date\") \\\n",
    "        .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ First and last transaction per user:\")\n",
    "    df.withColumn(\"first_amount\", first(\"amount\").over(user_date_window)) \\\n",
    "        .withColumn(\"last_amount\", last(\"amount\").over(user_date_window)) \\\n",
    "        .select(\"user_id\", \"amount\", \"first_amount\", \"last_amount\") \\\n",
    "        .distinct() \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 7: Practical example - Product analytics\n",
    "\n",
    "    \n",
    "    Combining what we've learned to solve a real analytics problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 7: Practical Example - Product Performance Analysis\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top performing products:\")\n",
    "    \n",
    "    # Product-level metrics\n",
    "    product_metrics = transactions_df \\\n",
    "        .filter(col(\"status\") == \"completed\") \\\n",
    "        .groupBy(\"product_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"num_sales\"),\n",
    "            spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_sale_price\"),\n",
    "            countDistinct(\"user_id\").alias(\"unique_buyers\")\n",
    "        )\n",
    "    \n",
    "    # Join with products to get product names\n",
    "    product_performance = product_metrics \\\n",
    "        .join(products_df, \"product_id\") \\\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"name\",\n",
    "            \"category\",\n",
    "            \"num_sales\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_sale_price\",\n",
    "            \"unique_buyers\"\n",
    "        ) \\\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    print(\"Overall product performance:\")\n",
    "    product_performance.show(10, truncate=False)\n",
    "    \n",
    "    # Rank products within each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top 3 products per category:\")\n",
    "    category_window = Window.partitionBy(\"category\").orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    product_performance \\\n",
    "        .withColumn(\"rank_in_category\", row_number().over(category_window)) \\\n",
    "        .filter(col(\"rank_in_category\") <= 3) \\\n",
    "        .select(\"category\", \"name\", \"total_revenue\", \"num_sales\", \"rank_in_category\") \\\n",
    "        .orderBy(\"category\", \"rank_in_category\") \\\n",
    "        .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Notebook completed! Check the next notebook to continue learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run to stop Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_spark_session(spark)\n",
    "# print(\"âœ… Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-pyspark-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
