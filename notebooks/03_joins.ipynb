{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Job 3: Joins and Data Relationships\n\n\nLearning Objectives:\n- Understanding different join types (inner, outer, left, right, cross)\n- Join strategies and performance considerations\n- Broadcast joins for small tables\n- Handling duplicate column names\n- Self joins for hierarchical data\n\nKey Concepts:\n- Joins are wide transformations (cause shuffle)\n- Broadcast joins avoid shuffle for small tables\n- Choose join types based on your data requirements\n- Proper join keys are critical for correctness and performance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Import Libraries and Initialize Spark"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add project root to path  \nproject_root = os.path.dirname(os.getcwd())\nsys.path.append(project_root)\n\nimport sys\nimport os\n\nprint(\"\u2705 Libraries imported successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Spark Session\nspark = get_spark_session(\"Job 3: Joins and Data Relationships\")\ndata_dir = get_data_dir()\n\nprint(f\"\u2705 Spark session created!\")\nprint(f\"\ud83d\udcca Spark UI: http://localhost:4040\")\nprint(f\"\ud83d\udcc1 Data directory: {data_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate sample data if it doesn't exist\nfrom utils.data_generator import generate_all_datasets\nimport os\n\ndata_files = os.path.join(data_dir, \"users.csv\")\nif not os.path.exists(data_files):\n    print(\"\ud83d\udcc1 Sample data not found. Generating...\")\n    generate_all_datasets(data_dir)\n    print(\"\u2705 Sample data generated!\")\nelse:\n    print(\"\u2705 Sample data already exists.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 1: Inner Join\n\n    \n    Inner join returns only rows that have matching values in both DataFrames.\n    This is the most common join type.\n    \n    Use when: You only want records that exist in both datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 1: Inner Join\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Inner join: Users with their transactions\")\n    print(\"   (Only users who have made transactions)\")\n    \n    # Basic inner join\n    result = users_df.join(\n        transactions_df,\n        users_df.user_id == transactions_df.user_id,\n        \"inner\"\n    )\n    \n    print(f\"\\nUsers: {users_df.count()} rows\")\n    print(f\"Transactions: {transactions_df.count()} rows\")\n    print(f\"After inner join: {result.count()} rows\")\n    \n    # Select specific columns to show\n    result.select(\n        users_df.user_id,\n        users_df.name,\n        users_df.email,\n        transactions_df.transaction_id,\n        transactions_df.amount,\n        transactions_df.transaction_date\n    ).show(10)\n    \n    # Count transactions per user"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Transactions per user (aggregated):\")\n    users_df.join(transactions_df, \"user_id\", \"inner\") \\\n        .groupBy(\"user_id\", \"name\") \\\n        .agg(\n            count(\"transaction_id\").alias(\"num_transactions\"),\n            spark_sum(\"amount\").alias(\"total_spent\")\n        ) \\\n        .orderBy(desc(\"total_spent\")) \\\n        .show(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 2: Left (Outer) Join\n\n    \n    Left join returns all rows from the left DataFrame, and matching rows\n    from the right. If no match, right columns will be null.\n    \n    Use when: You want all records from the left table, regardless of matches."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 2: Left Join\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Left join: All users, with transactions if they exist\")\n    \n    result = users_df.join(\n        transactions_df,\n        \"user_id\",\n        \"left\"\n    )\n    \n    print(f\"\\nUsers: {users_df.count()} rows\")\n    print(f\"After left join: {result.count()} rows\")\n    \n    # Show some users with and without transactions\n    result.select(\n        \"user_id\",\n        \"name\",\n        \"transaction_id\",\n        \"amount\"\n    ).show(15)\n    \n    # Find users with no transactions (nulls in transaction columns)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Users with NO transactions:\")\n    users_without_transactions = result \\\n        .filter(col(\"transaction_id\").isNull()) \\\n        .select(\"user_id\", \"name\", \"email\") \\\n        .distinct()\n    \n    print(f\"Found {users_without_transactions.count()} users with no transactions\")\n    users_without_transactions.show(10)\n    \n    # Count transactions per user (including zero)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 All users with transaction counts (including zero):\")\n    users_df.join(transactions_df, \"user_id\", \"left\") \\\n        .groupBy(\"user_id\", \"name\") \\\n        .agg(\n            count(\"transaction_id\").alias(\"num_transactions\"),\n            spark_sum(\"amount\").alias(\"total_spent\")\n        ) \\\n        .orderBy(\"user_id\") \\\n        .show(15)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 3: Right (Outer) Join\n\n    \n    Right join returns all rows from the right DataFrame, and matching rows\n    from the left. If no match, left columns will be null.\n    \n    Use when: You want all records from the right table.\n    (Note: Right join is less common; you can usually use left join instead)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 3: Right Join\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Right join: All transactions, with user info if available\")\n    \n    result = users_df.join(\n        transactions_df,\n        \"user_id\",\n        \"right\"\n    )\n    \n    print(f\"Transactions: {transactions_df.count()} rows\")\n    print(f\"After right join: {result.count()} rows\")\n    \n    result.select(\n        \"user_id\",\n        \"name\",\n        \"transaction_id\",\n        \"amount\"\n    ).show(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 4: Full Outer Join\n\n    \n    Full outer join returns all rows from both DataFrames. If no match,\n    the missing side will have nulls.\n    \n    Use when: You want all records from both tables, regardless of matches."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 4: Full Outer Join\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Full outer join: All users AND all transactions\")\n    \n    result = users_df.join(\n        transactions_df,\n        \"user_id\",\n        \"full\"  # or \"fullouter\"\n    )\n    \n    print(f\"Users: {users_df.count()} rows\")\n    print(f\"Transactions: {transactions_df.count()} rows\")\n    print(f\"After full outer join: {result.count()} rows\")\n    \n    # Count nulls on each side"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Analyzing the join result:\")\n    print(f\"Rows with missing user data: {result.filter(col('name').isNull()).count()}\")\n    print(f\"Rows with missing transaction data: {result.filter(col('transaction_id').isNull()).count()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 5: Cross Join\n\n    \n    Cross join returns the Cartesian product of two DataFrames.\n    Every row from the left is combined with every row from the right.\n    \n    WARNING: This can create huge results! Use with caution.\n    Use when: You need all possible combinations (e.g., product recommendations)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 5: Cross Join\")\n    print(\"=\"*70)\n    \n    # Create small sample data for demonstration\n    categories = [(\"Electronics\",), (\"Books\",), (\"Clothing\",)]\n    sizes = [(\"S\",), (\"M\",), (\"L\",)]\n    \n    categories_df = spark.createDataFrame(categories, [\"category\"])\n    sizes_df = spark.createDataFrame(sizes, [\"size\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Cross join: All category-size combinations\")\n    print(f\"Categories: {categories_df.count()} rows\")\n    print(f\"Sizes: {sizes_df.count()} rows\")\n    \n    result = categories_df.crossJoin(sizes_df)\n    print(f\"After cross join: {result.count()} rows (3 \u00d7 3 = 9)\")\n    \n    result.show()\n    \n    print(\"\\n\u26a0\ufe0f  WARNING: Cross joins can create massive results!\")\n    print(\"   1000 \u00d7 1000 = 1,000,000 rows\")\n    print(\"   Use only when you truly need all combinations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 6: Multiple Joins\n\n    \n    Real-world scenarios often require joining multiple DataFrames.\n    Chain joins to combine data from various sources."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 6: Multiple Joins\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Join users, transactions, and products together\")\n    \n    # Join transactions with users\n    transactions_with_users = transactions_df.join(\n        users_df,\n        \"user_id\",\n        \"inner\"\n    )\n    \n    # Then join with products\n    full_data = transactions_with_users.join(\n        products_df,\n        \"product_id\",\n        \"inner\"\n    )\n    \n    print(\"\\nComplete transaction view with user and product details:\")\n    full_data.select(\n        transactions_df.transaction_id,\n        users_df.name.alias(\"customer_name\"),\n        products_df.name.alias(\"product_name\"),\n        products_df.category,\n        transactions_df.amount,\n        transactions_df.transaction_date\n    ).show(10, truncate=False)\n    \n    # Practical analysis: Top spending customers by category"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Top spenders by product category:\")\n    full_data.filter(col(\"status\") == \"completed\") \\\n        .groupBy(\"category\", users_df.name) \\\n        .agg(\n            spark_sum(\"amount\").alias(\"total_spent\"),\n            count(\"transaction_id\").alias(\"num_purchases\")\n        ) \\\n        .orderBy(\"category\", desc(\"total_spent\")) \\\n        .show(20, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 7: Broadcast Join (Performance Optimization)\n\n    \n    Broadcast join is an optimization for joining a large DataFrame with\n    a small one. The small DataFrame is sent to all executors, avoiding shuffle.\n    \n    Use when:\n    - One DataFrame is small (< 10MB typically)\n    - You want to avoid expensive shuffle operations\n    \n    Rule of thumb: Broadcast if one side is < 100MB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 7: Broadcast Join (Performance Optimization)\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Regular join vs Broadcast join\")\n    \n    # Regular join (both DataFrames shuffled)\n    print(\"\\n1. Regular join:\")\n    regular_join = transactions_df.join(users_df, \"user_id\", \"inner\")\n    print(\"   Both DataFrames are shuffled across the cluster\")\n    \n    # Broadcast join (small DataFrame sent to all nodes)\n    print(\"\\n2. Broadcast join:\")\n    broadcast_join = transactions_df.join(\n        broadcast(users_df),  # Hint to broadcast users_df\n        \"user_id\",\n        \"inner\"\n    )\n    print(\"   Users DataFrame is copied to all executors (no shuffle!)\")\n    \n    # Show execution plans\n    print(\"\\n\ud83d\udcca Execution plans (use explain() to see the difference):\")\n    print(\"\\nRegular join plan:\")\n    regular_join.explain()\n    \n    print(\"\\n\" + \"-\"*70)\n    print(\"\\nBroadcast join plan:\")\n    broadcast_join.explain()\n    \n    print(\"\\n\ud83d\udca1 Broadcast joins are much faster for small lookup tables!\")\n    print(\"   Examples: country codes, product categories, user segments\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 8: Handling Duplicate Column Names\n\n    \n    When joining, both DataFrames might have columns with the same name.\n    Learn strategies to handle this."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 8: Handling Duplicate Column Names\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Problem: Both DataFrames have 'user_id' column\")\n    \n    # Strategy 1: Use the join column name directly (it's deduplicated)\n    print(\"\\n1. Join on the common column:\")\n    result1 = users_df.join(transactions_df, \"user_id\", \"inner\")\n    print(\"   The join column appears only once\")\n    result1.select(\"user_id\", \"name\", \"transaction_id\").show(5)\n    \n    # Strategy 2: Use explicit column references with DataFrame prefixes\n    print(\"\\n2. Use DataFrame prefixes for clarity:\")\n    result2 = users_df.alias(\"u\").join(\n        transactions_df.alias(\"t\"),\n        col(\"u.user_id\") == col(\"t.user_id\"),\n        \"inner\"\n    ).select(\n        col(\"u.user_id\").alias(\"user_id\"),\n        col(\"u.name\"),\n        col(\"t.transaction_id\"),\n        col(\"t.amount\")\n    )\n    result2.show(5)\n    \n    # Strategy 3: Rename columns before joining\n    print(\"\\n3. Rename columns to avoid conflicts:\")\n    transactions_renamed = transactions_df.withColumnRenamed(\"user_id\", \"customer_id\")\n    result3 = users_df.join(\n        transactions_renamed,\n        users_df.user_id == transactions_renamed.customer_id,\n        \"inner\"\n    )\n    result3.select(\"user_id\", \"customer_id\", \"name\", \"transaction_id\").show(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 9: Self Join\n\n    \n    Self join joins a DataFrame with itself. Useful for hierarchical data\n    or finding relationships within the same dataset.\n    \n    Example: Employee-Manager relationships, product recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 9: Self Join\")\n    print(\"=\"*70)\n    \n    # Create sample employee-manager data\n    employees = [\n        (1, \"Alice\", None),      # CEO, no manager\n        (2, \"Bob\", 1),           # Reports to Alice\n        (3, \"Charlie\", 1),       # Reports to Alice\n        (4, \"Diana\", 2),         # Reports to Bob\n        (5, \"Eve\", 2),           # Reports to Bob\n        (6, \"Frank\", 3),         # Reports to Charlie\n    ]\n    \n    emp_df = spark.createDataFrame(employees, [\"emp_id\", \"emp_name\", \"manager_id\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Employee hierarchy:\")\n    emp_df.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Self join: Employees with their manager names\")\n    # Join employees with themselves to get manager names\n    result = emp_df.alias(\"emp\").join(\n        emp_df.alias(\"mgr\"),\n        col(\"emp.manager_id\") == col(\"mgr.emp_id\"),\n        \"left\"  # Left join to include CEO (no manager)\n    ).select(\n        col(\"emp.emp_id\"),\n        col(\"emp.emp_name\").alias(\"employee\"),\n        col(\"mgr.emp_name\").alias(\"manager\")\n    )\n    \n    result.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LESSON 10: Practical Example - Customer 360 View\n\n    \n    Combine everything to create a comprehensive customer analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\n    print(\"LESSON 10: Practical Example - Customer 360 View\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Building a comprehensive customer profile:\")\n    \n    # Join all data\n    customer_360 = transactions_df \\\n        .filter(col(\"status\") == \"completed\") \\\n        .join(users_df, \"user_id\", \"inner\") \\\n        .join(products_df, \"product_id\", \"inner\")\n    \n    # Aggregate to customer level\n    customer_profile = customer_360.groupBy(\n        \"user_id\",\n        \"name\",\n        \"email\",\n        \"country\",\n        \"signup_date\"\n    ).agg(\n        count(\"transaction_id\").alias(\"total_purchases\"),\n        spark_sum(\"amount\").alias(\"lifetime_value\"),\n        spark_sum(col(\"quantity\")).alias(\"total_items\"),\n        count(col(\"category\").distinct()).alias(\"categories_purchased\")\n    ).orderBy(desc(\"lifetime_value\"))\n    \n    print(\"\\nTop customers by lifetime value:\")\n    customer_profile.show(10, truncate=False)\n    \n    # Category preferences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udd39 Customer category preferences:\")\n    category_prefs = customer_360.groupBy(\"user_id\", \"name\", \"category\") \\\n        .agg(\n            count(\"*\").alias(\"purchases\"),\n            spark_sum(\"amount\").alias(\"spent\")\n        ) \\\n        .orderBy(\"user_id\", desc(\"spent\"))\n    \n    category_prefs.show(20, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n\u2705 Notebook completed! Check the next notebook to continue learning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cleanup (Optional)\n\nUncomment and run to stop Spark session:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# stop_spark_session(spark)\n# print(\"\u2705 Spark session stopped.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}