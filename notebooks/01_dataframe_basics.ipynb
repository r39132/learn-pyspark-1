{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e2850c",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Spark\n",
    "\n",
    "**âš ï¸ Important**: If you get a `FileNotFoundError` about `./bin/spark-submit`, you need to:\n",
    "1. Ensure `JAVA_HOME` is set in your `.env` file (see README)\n",
    "2. **Restart the kernel** (Kernel â†’ Restart Kernel)\n",
    "3. Run this cell again to load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc32d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JAVA_HOME set to: /Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home\n",
      "âœ… Libraries imported successfully!\n",
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set JAVA_HOME before any PySpark imports\n",
    "# This must be the very first thing we do\n",
    "java_home = '/Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home'\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "print(f\"âœ… JAVA_HOME set to: {java_home}\")\n",
    "\n",
    "# Also set SPARK_HOME if needed (PySpark will find it automatically, but let's be explicit)\n",
    "import site\n",
    "site_packages = site.getsitepackages()[0]\n",
    "spark_home = os.path.join(site_packages, 'pyspark')\n",
    "if os.path.exists(spark_home):\n",
    "    os.environ['SPARK_HOME'] = spark_home\n",
    "    print(f\"âœ… SPARK_HOME set to: {spark_home}\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load other environment variables from .env file\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path, override=False)  # Don't override JAVA_HOME we just set\n",
    "    print(f\"âœ… Loaded environment from: {env_path}\")\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# NOW import PySpark - JAVA_HOME is already set\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, upper, lower, concat, when, year, month\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Import project utilities\n",
    "from utils.spark_session import get_spark_session, stop_spark_session, get_data_dir\n",
    "from utils.data_generator import generate_all_datasets\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6a42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Java is accessible:\n",
      "openjdk version \"17.0.8.1\" 2023-08-22 LTS\n"
     ]
    }
   ],
   "source": [
    "# Verify Java is accessible\n",
    "import subprocess\n",
    "try:\n",
    "    java_version = subprocess.run(\n",
    "        [os.path.join(os.environ['JAVA_HOME'], 'bin', 'java'), '-version'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    print(\"âœ… Java is accessible:\")\n",
    "    print(java_version.stderr.split('\\n')[0])\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error accessing Java: {e}\")\n",
    "    print(f\"   JAVA_HOME: {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n",
    "    print(f\"   Try: export JAVA_HOME=$(jenv prefix 17.0.8.1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b17ee79",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m spark = \u001b[43mget_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mJob 1: DataFrame Basics\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m data_dir = get_data_dir()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Spark session created!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/utils/spark_session.py:40\u001b[39m, in \u001b[36mget_spark_session\u001b[39m\u001b[34m(app_name, local_mode)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Suppress excessive logging (set to INFO, WARN, or ERROR)\u001b[39;00m\n\u001b[32m     37\u001b[39m builder = builder.config(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.extraJavaOptions\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     38\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33m-Dlog4j.configuration=file:log4j.properties\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m spark = \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Set log level to WARN to reduce console output\u001b[39;00m\n\u001b[32m     43\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py:97\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m         signal.signal(signal.SIGINT, signal.SIG_IGN)\n\u001b[32m     96\u001b[39m     popen_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpreexec_fn\u001b[39m\u001b[33m\"\u001b[39m] = preexec_func\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     proc = \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[32m    100\u001b[39m     proc = Popen(command, **popen_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/subprocess.py:1950\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1948\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errno_num != \u001b[32m0\u001b[39m:\n\u001b[32m   1949\u001b[39m         err_msg = os.strerror(errno_num)\n\u001b[32m-> \u001b[39m\u001b[32m1950\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1951\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Job 1: DataFrame Basics\")\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "print(f\"âœ… Spark session created!\")\n",
    "print(f\"ðŸ“Š Spark UI: http://localhost:4040\")\n",
    "print(f\"ðŸ“ Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8a41ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate sample data if it doesn't exist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m users_csv = os.path.join(\u001b[43mdata_dir\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33musers.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(users_csv):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“ Sample data not found. Generating...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "users_csv = os.path.join(data_dir, \"users.csv\")\n",
    "if not os.path.exists(users_csv):\n",
    "    print(\"ðŸ“ Sample data not found. Generating...\")\n",
    "    generate_all_datasets(data_dir)\n",
    "    print(\"âœ… Sample data generated!\")\n",
    "else:\n",
    "    print(\"âœ… Sample data already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071dc7cb",
   "metadata": {},
   "source": [
    "## LESSON 1: Creating DataFrames from Lists\n",
    "\n",
    "This is useful for quick testing and learning, but in production you'll typically read from files or databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed511e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 28, \"New York\"),\n",
    "    (2, \"Bob\", 35, \"San Francisco\"),\n",
    "    (3, \"Charlie\", 42, \"Seattle\"),\n",
    "    (4, \"Diana\", 31, \"Boston\")\n",
    "]\n",
    "\n",
    "# Create DataFrame with column names\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"city\"])\n",
    "\n",
    "print(\"ðŸ“Š Simple DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Print schema - shows column names and types\n",
    "print(\"ðŸ“‹ Schema (inferred automatically):\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48751785",
   "metadata": {},
   "source": [
    "## LESSON 2: Creating DataFrames with Explicit Schema\n",
    "\n",
    "Defining schema explicitly is better for:\n",
    "- Performance (no inference needed)\n",
    "- Type safety (you control the types)\n",
    "- Clarity (self-documenting code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),        # False = not nullable\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),        # True = nullable\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 28, 75000.0),\n",
    "    (2, \"Bob\", 35, 85000.0),\n",
    "    (3, \"Charlie\", None, 92000.0),  # Note: age is nullable\n",
    "    (4, \"Diana\", 31, None)           # salary is nullable\n",
    "]\n",
    "\n",
    "df_schema = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"ðŸ“Š DataFrame with explicit schema:\")\n",
    "df_schema.show()\n",
    "\n",
    "print(\"ðŸ“‹ Schema (explicitly defined):\")\n",
    "df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e897b2d",
   "metadata": {},
   "source": [
    "## LESSON 3: Reading Data from CSV Files\n",
    "\n",
    "CSV is common but not the most efficient format. Always specify options like header, inferSchema, delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = os.path.join(data_dir, \"users.csv\")\n",
    "\n",
    "# Read CSV with options\n",
    "users_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(users_path)\n",
    "\n",
    "print(f\"ðŸ“ Read from: {users_path}\")\n",
    "print(f\"ðŸ“Š Row count: {users_df.count()}\")\n",
    "print(\"\\nðŸ” First 10 rows:\")\n",
    "users_df.show(10)\n",
    "\n",
    "print(\"ðŸ“‹ Inferred schema:\")\n",
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f8f8f",
   "metadata": {},
   "source": [
    "## LESSON 4: Basic DataFrame Transformations\n",
    "\n",
    "Key transformations:\n",
    "- `select()`: Choose specific columns\n",
    "- `filter()`/`where()`: Filter rows based on conditions\n",
    "- `withColumn()`: Add or modify columns\n",
    "- `drop()`: Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01666032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT: Choose specific columns\n",
    "print(\"ðŸ”¹ select() - Choose columns:\")\n",
    "users_df.select(\"name\", \"email\", \"age\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT with column expressions\n",
    "print(\"ðŸ”¹ select() with expressions:\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"city\"),\n",
    "    col(\"country\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12470c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER: Keep only rows that match condition\n",
    "print(\"ðŸ”¹ filter() - Users age > 30:\")\n",
    "users_df.filter(col(\"age\") > 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple filter conditions (AND)\n",
    "print(\"ðŸ”¹ filter() with multiple conditions (AND):\")\n",
    "users_df.filter(\n",
    "    (col(\"age\") > 25) & (col(\"country\") == \"USA\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple filter conditions (OR)\n",
    "print(\"ðŸ”¹ filter() with OR condition:\")\n",
    "users_df.filter(\n",
    "    (col(\"country\") == \"USA\") | (col(\"country\") == \"UK\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5571927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE: Alternative to filter (same functionality)\n",
    "print(\"ðŸ”¹ where() - Same as filter:\")\n",
    "users_df.where(\"age > 40\").show(5)  # Can use SQL-like strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32636e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH_COLUMN: Add new column or modify existing\n",
    "print(\"ðŸ”¹ withColumn() - Add age_group column:\")\n",
    "df_with_group = users_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "    .when(col(\"age\") < 50, \"Middle\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "df_with_group.select(\"name\", \"age\", \"age_group\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple withColumn operations (chaining)\n",
    "print(\"ðŸ”¹ Chain multiple withColumn():\")\n",
    "df_enhanced = users_df \\\n",
    "    .withColumn(\"full_name\", upper(col(\"name\"))) \\\n",
    "    .withColumn(\"email_lower\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"age_plus_10\", col(\"age\") + 10)\n",
    "\n",
    "df_enhanced.select(\"name\", \"full_name\", \"age\", \"age_plus_10\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP: Remove columns\n",
    "print(\"ðŸ”¹ drop() - Remove columns:\")\n",
    "df_reduced = users_df.drop(\"user_id\", \"signup_date\")\n",
    "print(f\"Original columns: {len(users_df.columns)}, After drop: {len(df_reduced.columns)}\")\n",
    "df_reduced.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc4937",
   "metadata": {},
   "source": [
    "## LESSON 5: Column Operations and Expressions\n",
    "\n",
    "Column operations are the heart of DataFrame transformations. Learn different ways to reference and manipulate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3294d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column arithmetic\n",
    "print(\"ðŸ”¹ Column arithmetic:\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    (col(\"age\") + 10).alias(\"age_in_10_years\"),\n",
    "    (col(\"age\") * 12).alias(\"age_in_months\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String operations\n",
    "print(\"ðŸ”¹ String operations:\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"name_upper\"),\n",
    "    lower(col(\"name\")).alias(\"name_lower\"),\n",
    "    concat(col(\"name\"), lit(\" - \"), col(\"city\")).alias(\"name_city\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional expressions with when/otherwise\n",
    "print(\"ðŸ”¹ Conditional expressions (when/otherwise):\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"country\"),\n",
    "    when(col(\"country\") == \"USA\", \"Domestic\")\n",
    "    .when(col(\"country\") == \"Canada\", \"Domestic\")\n",
    "    .otherwise(\"International\")\n",
    "    .alias(\"market\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with dates (if available)\n",
    "if \"signup_date\" in users_df.columns:\n",
    "    print(\"ðŸ”¹ Date operations:\")\n",
    "    users_df.select(\n",
    "        col(\"name\"),\n",
    "        col(\"signup_date\"),\n",
    "        year(col(\"signup_date\")).alias(\"signup_year\"),\n",
    "        month(col(\"signup_date\")).alias(\"signup_month\")\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20df79",
   "metadata": {},
   "source": [
    "## LESSON 6: Reading and Writing Different Formats\n",
    "\n",
    "Formats:\n",
    "- **CSV**: Human-readable, but slow and not type-safe\n",
    "- **JSON**: Good for nested data, but verbose\n",
    "- **Parquet**: Columnar format, fast, compressed (RECOMMENDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON\n",
    "print(\"ðŸ”¹ Reading JSON:\")\n",
    "products_df = spark.read.option(\"multiLine\", \"true\").json(os.path.join(data_dir, \"products.json\"))\n",
    "print(f\"Products JSON: {products_df.count()} rows\")\n",
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27538aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write as Parquet (best practice for Spark)\n",
    "print(\"ðŸ”¹ Writing as Parquet:\")\n",
    "output_path = os.path.join(data_dir, \"users_parquet\")\n",
    "users_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)\n",
    "print(f\"âœ“ Written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752808ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from Parquet\n",
    "print(\"ðŸ”¹ Reading Parquet:\")\n",
    "users_parquet = spark.read.parquet(output_path)\n",
    "print(f\"Read back: {users_parquet.count()} rows\")\n",
    "users_parquet.show(5)\n",
    "\n",
    "# Compare file sizes (Parquet is much smaller)\n",
    "csv_size = os.path.getsize(os.path.join(data_dir, \"users.csv\"))\n",
    "print(f\"\\nðŸ“¦ CSV size: {csv_size:,} bytes\")\n",
    "print(\"   Parquet is typically 5-10x smaller and much faster to read!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing with partitioning (important for large datasets)\n",
    "print(\"ðŸ”¹ Writing with partitioning:\")\n",
    "partitioned_path = os.path.join(data_dir, \"users_partitioned\")\n",
    "users_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\") \\\n",
    "    .parquet(partitioned_path)\n",
    "print(f\"âœ“ Written partitioned data to: {partitioned_path}\")\n",
    "print(\"   Partitioning helps with query performance on large datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c08a58",
   "metadata": {},
   "source": [
    "## LESSON 7: DataFrame Actions (Trigger Computation)\n",
    "\n",
    "Actions trigger computation. Transformations are lazy, actions are eager.\n",
    "\n",
    "Common actions:\n",
    "- `show()`: Display data\n",
    "- `count()`: Count rows\n",
    "- `collect()`: Bring all data to driver (BE CAREFUL!)\n",
    "- `take()`: Take first N rows\n",
    "- `first()`: Get first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ show() - Display data:\")\n",
    "users_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ count() - Count rows:\")\n",
    "row_count = users_df.count()\n",
    "print(f\"Total rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8442147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ first() - Get first row:\")\n",
    "first_row = users_df.first()\n",
    "print(f\"First row: {first_row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ea044",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ take() - Get first N rows:\")\n",
    "first_3 = users_df.take(3)\n",
    "print(f\"First 3 rows: {len(first_3)} rows\")\n",
    "for row in first_3:\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50882b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ describe() - Statistical summary:\")\n",
    "users_df.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c235b0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… What You Learned:\n",
    "- âœ“ Creating DataFrames from lists and files\n",
    "- âœ“ Defining explicit schemas\n",
    "- âœ“ Basic transformations: select, filter, withColumn\n",
    "- âœ“ Column operations and expressions\n",
    "- âœ“ Reading and writing different formats (CSV, JSON, Parquet)\n",
    "- âœ“ Understanding actions vs transformations\n",
    "\n",
    "### ðŸŽ¯ Next Step:\n",
    "Run Notebook 2 to learn about aggregations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd5aec",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell to stop the Spark session when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b782c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_spark_session(spark)\n",
    "# print(\"âœ… Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-pyspark-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
