{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e2850c",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Spark\n",
    "\n",
    "**âš ï¸ Important**: If you get a `FileNotFoundError` about `./bin/spark-submit`, you need to:\n",
    "1. Ensure `JAVA_HOME` is set in your `.env` file (see README)\n",
    "2. **Restart the kernel** (Kernel â†’ Restart Kernel)\n",
    "3. Run this cell again to load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc32d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JAVA_HOME set to: /Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home\n",
      "âœ… SPARK_HOME set to: /Users/r39132/Projects/learn-pyspark-1/.venv/lib/python3.12/site-packages/pyspark\n",
      "âœ… Loaded environment from: /Users/r39132/Projects/learn-pyspark-1/.env\n",
      "âœ… Libraries imported successfully!\n",
      "   Python: 3.12.0\n",
      "   Project root: /Users/r39132/Projects/learn-pyspark-1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set JAVA_HOME before any PySpark imports\n",
    "# This must be the very first thing we do\n",
    "java_home = '/Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home'\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "print(f\"âœ… JAVA_HOME set to: {java_home}\")\n",
    "\n",
    "# Also set SPARK_HOME if needed (PySpark will find it automatically, but let's be explicit)\n",
    "import site\n",
    "site_packages = site.getsitepackages()[0]\n",
    "spark_home = os.path.join(site_packages, 'pyspark')\n",
    "if os.path.exists(spark_home):\n",
    "    os.environ['SPARK_HOME'] = spark_home\n",
    "    print(f\"âœ… SPARK_HOME set to: {spark_home}\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load other environment variables from .env file\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path, override=False)  # Don't override JAVA_HOME we just set\n",
    "    print(f\"âœ… Loaded environment from: {env_path}\")\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# NOW import PySpark - JAVA_HOME is already set\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, upper, lower, concat, when, year, month\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Import project utilities\n",
    "from utils.spark_session import get_spark_session, stop_spark_session, get_data_dir\n",
    "from utils.data_generator import generate_all_datasets\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6a42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Java is accessible:\n",
      "openjdk version \"17.0.8.1\" 2023-08-22 LTS\n"
     ]
    }
   ],
   "source": [
    "# Verify Java is accessible\n",
    "import subprocess\n",
    "try:\n",
    "    java_version = subprocess.run(\n",
    "        [os.path.join(os.environ['JAVA_HOME'], 'bin', 'java'), '-version'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    print(\"âœ… Java is accessible:\")\n",
    "    print(java_version.stderr.split('\\n')[0])\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error accessing Java: {e}\")\n",
    "    print(f\"   JAVA_HOME: {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n",
    "    print(f\"   Try: export JAVA_HOME=$(jenv prefix 17.0.8.1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Job 1: DataFrame Basics\")\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "print(f\"âœ… Spark session created!\")\n",
    "print(f\"ðŸ“Š Spark UI: http://localhost:4040\")\n",
    "print(f\"ðŸ“ Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "users_csv = os.path.join(data_dir, \"users.csv\")\n",
    "if not os.path.exists(users_csv):\n",
    "    print(\"ðŸ“ Sample data not found. Generating...\")\n",
    "    generate_all_datasets(data_dir)\n",
    "    print(\"âœ… Sample data generated!\")\n",
    "else:\n",
    "    print(\"âœ… Sample data already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071dc7cb",
   "metadata": {},
   "source": [
    "## LESSON 1: Creating DataFrames from Lists\n",
    "\n",
    "This is useful for quick testing and learning, but in production you'll typically read from files or databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed511e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 28, \"New York\"),\n",
    "    (2, \"Bob\", 35, \"San Francisco\"),\n",
    "    (3, \"Charlie\", 42, \"Seattle\"),\n",
    "    (4, \"Diana\", 31, \"Boston\")\n",
    "]\n",
    "\n",
    "# Create DataFrame with column names\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"city\"])\n",
    "\n",
    "print(\"ðŸ“Š Simple DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Print schema - shows column names and types\n",
    "print(\"ðŸ“‹ Schema (inferred automatically):\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48751785",
   "metadata": {},
   "source": [
    "## LESSON 2: Creating DataFrames with Explicit Schema\n",
    "\n",
    "Defining schema explicitly is better for:\n",
    "- Performance (no inference needed)\n",
    "- Type safety (you control the types)\n",
    "- Clarity (self-documenting code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),        # False = not nullable\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),        # True = nullable\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 28, 75000.0),\n",
    "    (2, \"Bob\", 35, 85000.0),\n",
    "    (3, \"Charlie\", None, 92000.0),  # Note: age is nullable\n",
    "    (4, \"Diana\", 31, None)           # salary is nullable\n",
    "]\n",
    "\n",
    "df_schema = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"ðŸ“Š DataFrame with explicit schema:\")\n",
    "df_schema.show()\n",
    "\n",
    "print(\"ðŸ“‹ Schema (explicitly defined):\")\n",
    "df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e897b2d",
   "metadata": {},
   "source": [
    "## LESSON 3: Reading Data from CSV Files\n",
    "\n",
    "CSV is common but not the most efficient format. Always specify options like header, inferSchema, delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = os.path.join(data_dir, \"users.csv\")\n",
    "\n",
    "# Read CSV with options\n",
    "users_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(users_path)\n",
    "\n",
    "print(f\"ðŸ“ Read from: {users_path}\")\n",
    "print(f\"ðŸ“Š Row count: {users_df.count()}\")\n",
    "print(\"\\nðŸ” First 10 rows:\")\n",
    "users_df.show(10)\n",
    "\n",
    "print(\"ðŸ“‹ Inferred schema:\")\n",
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f8f8f",
   "metadata": {},
   "source": [
    "## LESSON 4: Basic DataFrame Transformations\n",
    "\n",
    "Key transformations:\n",
    "- `select()`: Choose specific columns\n",
    "- `filter()`/`where()`: Filter rows based on conditions\n",
    "- `withColumn()`: Add or modify columns\n",
    "- `drop()`: Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01666032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT: Choose specific columns\n",
    "print(\"ðŸ”¹ select() - Choose columns:\")\n",
    "users_df.select(\"name\", \"email\", \"age\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT with column expressions\n",
    "print(\"ðŸ”¹ select() with expressions:\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"city\"),\n",
    "    col(\"country\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12470c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER: Keep only rows that match condition\n",
    "print(\"ðŸ”¹ filter() - Users age > 30:\")\n",
    "users_df.filter(col(\"age\") > 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple filter conditions (AND)\n",
    "print(\"ðŸ”¹ filter() with multiple conditions (AND):\")\n",
    "users_df.filter(\n",
    "    (col(\"age\") > 25) & (col(\"country\") == \"USA\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple filter conditions (OR)\n",
    "print(\"ðŸ”¹ filter() with OR condition:\")\n",
    "users_df.filter(\n",
    "    (col(\"country\") == \"USA\") | (col(\"country\") == \"UK\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5571927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE: Alternative to filter (same functionality)\n",
    "print(\"ðŸ”¹ where() - Same as filter:\")\n",
    "users_df.where(\"age > 40\").show(5)  # Can use SQL-like strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32636e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH_COLUMN: Add new column or modify existing\n",
    "print(\"ðŸ”¹ withColumn() - Add age_group column:\")\n",
    "df_with_group = users_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "    .when(col(\"age\") < 50, \"Middle\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "df_with_group.select(\"name\", \"age\", \"age_group\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d674f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple withColumn operations (chaining)\n",
    "print(\"ðŸ”¹ Chain multiple withColumn():\")\n",
    "df_enhanced = users_df \\\n",
    "    .withColumn(\"full_name\", upper(col(\"name\"))) \\\n",
    "    .withColumn(\"email_lower\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"age_plus_10\", col(\"age\") + 10)\n",
    "\n",
    "df_enhanced.select(\"name\", \"full_name\", \"age\", \"age_plus_10\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP: Remove columns\n",
    "print(\"ðŸ”¹ drop() - Remove columns:\")\n",
    "df_reduced = users_df.drop(\"user_id\", \"signup_date\")\n",
    "print(f\"Original columns: {len(users_df.columns)}, After drop: {len(df_reduced.columns)}\")\n",
    "df_reduced.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc4937",
   "metadata": {},
   "source": [
    "## LESSON 5: Column Operations and Expressions\n",
    "\n",
    "Column operations are the heart of DataFrame transformations. Learn different ways to reference and manipulate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3294d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column arithmetic\n",
    "print(\"ðŸ”¹ Column arithmetic:\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    (col(\"age\") + 10).alias(\"age_in_10_years\"),\n",
    "    (col(\"age\") * 12).alias(\"age_in_months\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String operations\n",
    "print(\"ðŸ”¹ String operations:\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"name_upper\"),\n",
    "    lower(col(\"name\")).alias(\"name_lower\"),\n",
    "    concat(col(\"name\"), lit(\" - \"), col(\"city\")).alias(\"name_city\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional expressions with when/otherwise\n",
    "print(\"ðŸ”¹ Conditional expressions (when/otherwise):\")\n",
    "users_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"country\"),\n",
    "    when(col(\"country\") == \"USA\", \"Domestic\")\n",
    "    .when(col(\"country\") == \"Canada\", \"Domestic\")\n",
    "    .otherwise(\"International\")\n",
    "    .alias(\"market\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with dates (if available)\n",
    "if \"signup_date\" in users_df.columns:\n",
    "    print(\"ðŸ”¹ Date operations:\")\n",
    "    users_df.select(\n",
    "        col(\"name\"),\n",
    "        col(\"signup_date\"),\n",
    "        year(col(\"signup_date\")).alias(\"signup_year\"),\n",
    "        month(col(\"signup_date\")).alias(\"signup_month\")\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20df79",
   "metadata": {},
   "source": [
    "## LESSON 6: Reading and Writing Different Formats\n",
    "\n",
    "Formats:\n",
    "- **CSV**: Human-readable, but slow and not type-safe\n",
    "- **JSON**: Good for nested data, but verbose\n",
    "- **Parquet**: Columnar format, fast, compressed (RECOMMENDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON\n",
    "print(\"ðŸ”¹ Reading JSON:\")\n",
    "products_df = spark.read.option(\"multiLine\", \"true\").json(os.path.join(data_dir, \"products.json\"))\n",
    "print(f\"Products JSON: {products_df.count()} rows\")\n",
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27538aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write as Parquet (best practice for Spark)\n",
    "print(\"ðŸ”¹ Writing as Parquet:\")\n",
    "output_path = os.path.join(data_dir, \"users_parquet\")\n",
    "users_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)\n",
    "print(f\"âœ“ Written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752808ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from Parquet\n",
    "print(\"ðŸ”¹ Reading Parquet:\")\n",
    "users_parquet = spark.read.parquet(output_path)\n",
    "print(f\"Read back: {users_parquet.count()} rows\")\n",
    "users_parquet.show(5)\n",
    "\n",
    "# Compare file sizes (Parquet is much smaller)\n",
    "csv_size = os.path.getsize(os.path.join(data_dir, \"users.csv\"))\n",
    "print(f\"\\nðŸ“¦ CSV size: {csv_size:,} bytes\")\n",
    "print(\"   Parquet is typically 5-10x smaller and much faster to read!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing with partitioning (important for large datasets)\n",
    "print(\"ðŸ”¹ Writing with partitioning:\")\n",
    "partitioned_path = os.path.join(data_dir, \"users_partitioned\")\n",
    "users_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\") \\\n",
    "    .parquet(partitioned_path)\n",
    "print(f\"âœ“ Written partitioned data to: {partitioned_path}\")\n",
    "print(\"   Partitioning helps with query performance on large datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c08a58",
   "metadata": {},
   "source": [
    "## LESSON 7: DataFrame Actions (Trigger Computation)\n",
    "\n",
    "Actions trigger computation. Transformations are lazy, actions are eager.\n",
    "\n",
    "Common actions:\n",
    "- `show()`: Display data\n",
    "- `count()`: Count rows\n",
    "- `collect()`: Bring all data to driver (BE CAREFUL!)\n",
    "- `take()`: Take first N rows\n",
    "- `first()`: Get first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ show() - Display data:\")\n",
    "users_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ count() - Count rows:\")\n",
    "row_count = users_df.count()\n",
    "print(f\"Total rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8442147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ first() - Get first row:\")\n",
    "first_row = users_df.first()\n",
    "print(f\"First row: {first_row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ea044",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ take() - Get first N rows:\")\n",
    "first_3 = users_df.take(3)\n",
    "print(f\"First 3 rows: {len(first_3)} rows\")\n",
    "for row in first_3:\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50882b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ describe() - Statistical summary:\")\n",
    "users_df.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c235b0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… What You Learned:\n",
    "- âœ“ Creating DataFrames from lists and files\n",
    "- âœ“ Defining explicit schemas\n",
    "- âœ“ Basic transformations: select, filter, withColumn\n",
    "- âœ“ Column operations and expressions\n",
    "- âœ“ Reading and writing different formats (CSV, JSON, Parquet)\n",
    "- âœ“ Understanding actions vs transformations\n",
    "\n",
    "### ðŸŽ¯ Next Step:\n",
    "Run Notebook 2 to learn about aggregations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd5aec",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run this cell to stop the Spark session when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b782c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_spark_session(spark)\n",
    "# print(\"âœ… Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
