{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job 5: Search Indexing and Text Processing\n",
    "\n",
    "\n",
    "Learning Objectives:\n",
    "- Text processing and tokenization\n",
    "- Building inverted indexes for search\n",
    "- TF-IDF scoring for relevance\n",
    "- Product recommendations\n",
    "- Text similarity and matching\n",
    "\n",
    "Key Concepts:\n",
    "- Inverted indexes map terms to documents\n",
    "- TF-IDF measures term importance\n",
    "- Text processing is essential for search and NLP\n",
    "- Spark handles large-scale text processing efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# CRITICAL: Set JAVA_HOME before any PySpark imports\n",
    "java_home = '/Library/Java/JavaVirtualMachines/amazon-corretto-17.jdk/Contents/Home'\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(env_path, override=False)\n",
    "\n",
    "# Add project root to path  \n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from utils.spark_session import get_spark_session, stop_spark_session, get_data_dir\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Job 5: Search Indexing and Text Processing\")\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "print(f\"âœ… Spark session created!\")\n",
    "print(f\"ðŸ“Š Spark UI: http://localhost:4040\")\n",
    "print(f\"ðŸ“ Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "from utils.data_generator import generate_all_datasets\n",
    "import os\n",
    "\n",
    "data_files = os.path.join(data_dir, \"users.csv\")\n",
    "if not os.path.exists(data_files):\n",
    "    print(\"ðŸ“ Sample data not found. Generating...\")\n",
    "    generate_all_datasets(data_dir)\n",
    "    print(\"âœ… Sample data generated!\")\n",
    "else:\n",
    "    print(\"âœ… Sample data already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 1: Text Preprocessing\n",
    "\n",
    "    \n",
    "    Clean and normalize text data for analysis:\n",
    "    - Convert to lowercase\n",
    "    - Remove punctuation\n",
    "    - Tokenize (split into words)\n",
    "    - Remove stop words (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 1: Text Preprocessing\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Original review texts:\")\n",
    "    reviews_df.select(\"review_id\", \"review_text\").show(5, truncate=False)\n",
    "    \n",
    "    # Step 1: Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 1: Lowercase\")\n",
    "    reviews_clean = reviews_df.withColumn(\n",
    "        \"text_lower\",\n",
    "        lower(col(\"review_text\"))\n",
    "    )\n",
    "    reviews_clean.select(\"review_text\", \"text_lower\").show(3, truncate=False)\n",
    "    \n",
    "    # Step 2: Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 2: Remove punctuation\")\n",
    "    reviews_clean = reviews_clean.withColumn(\n",
    "        \"text_clean\",\n",
    "        regexp_replace(col(\"text_lower\"), \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    )\n",
    "    reviews_clean.select(\"text_lower\", \"text_clean\").show(3, truncate=False)\n",
    "    \n",
    "    # Step 3: Tokenize (split into words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 3: Tokenization (split into words)\")\n",
    "    reviews_clean = reviews_clean.withColumn(\n",
    "        \"tokens\",\n",
    "        split(trim(col(\"text_clean\")), \"\\\\s+\")\n",
    "    )\n",
    "    reviews_clean.select(\"text_clean\", \"tokens\").show(3, truncate=False)\n",
    "    \n",
    "    # Step 4: Filter out common stop words (simplified)\n",
    "    stop_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \n",
    "                  \"of\", \"with\", \"is\", \"was\", \"it\", \"this\", \"that\"]\n",
    "    \n",
    "    @udf(ArrayType(StringType()))\n",
    "    def remove_stop_words(tokens):\n",
    "        \"\"\"Remove common stop words\"\"\"\n",
    "        if tokens:\n",
    "            return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 4: Remove stop words and short words\")\n",
    "    reviews_clean = reviews_clean.withColumn(\n",
    "        \"filtered_tokens\",\n",
    "        remove_stop_words(col(\"tokens\"))\n",
    "    )\n",
    "    reviews_clean.select(\"tokens\", \"filtered_tokens\").show(3, truncate=False)\n",
    "    \n",
    "    return reviews_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 2: Building an Inverted Index\n",
    "\n",
    "    \n",
    "    An inverted index maps each word to the documents (reviews) containing it.\n",
    "    This is the foundation of search engines.\n",
    "    \n",
    "    Structure: word -> [list of document IDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 2: Building an Inverted Index\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Creating inverted index from reviews:\")\n",
    "    \n",
    "    # Preprocess text\n",
    "    reviews_processed = reviews_df \\\n",
    "        .withColumn(\"text_clean\", lower(regexp_replace(col(\"review_text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))) \\\n",
    "        .withColumn(\"tokens\", split(trim(col(\"text_clean\")), \"\\\\s+\"))\n",
    "    \n",
    "    # Explode tokens to get one row per word per document\n",
    "    word_document_pairs = reviews_processed \\\n",
    "        .select(\n",
    "            col(\"review_id\").alias(\"doc_id\"),\n",
    "            explode(col(\"tokens\")).alias(\"word\")\n",
    "        ) \\\n",
    "        .filter(length(col(\"word\")) > 2)  # Filter short words\n",
    "    \n",
    "    print(\"\\nWord-Document pairs (sample):\")\n",
    "    word_document_pairs.show(10)\n",
    "    \n",
    "    # Build inverted index: group by word, collect document IDs\n",
    "    inverted_idx = word_document_pairs \\\n",
    "        .groupBy(\"word\") \\\n",
    "        .agg(\n",
    "            collect_set(\"doc_id\").alias(\"doc_ids\"),\n",
    "            count(\"doc_id\").alias(\"doc_frequency\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"doc_frequency\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Inverted Index (top words):\")\n",
    "    print(\"Format: word -> [list of document IDs that contain it]\")\n",
    "    inverted_idx.show(20, truncate=False)\n",
    "    \n",
    "    # Search function using inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Search for 'great':\")\n",
    "    search_result = inverted_idx.filter(col(\"word\") == \"great\")\n",
    "    if search_result.count() > 0:\n",
    "        docs = search_result.first()[\"doc_ids\"]\n",
    "        print(f\"Found in {len(docs)} documents: {docs[:10]}\")\n",
    "    \n",
    "    return inverted_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 3: Term Frequency (TF)\n",
    "\n",
    "    \n",
    "    TF measures how often a term appears in a document.\n",
    "    Common formula: TF = (count of term in document) / (total terms in document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 3: Term Frequency (TF)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Calculating term frequency for each word in each document:\")\n",
    "    \n",
    "    # Preprocess\n",
    "    reviews_processed = reviews_df \\\n",
    "        .withColumn(\"text_clean\", lower(regexp_replace(col(\"review_text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))) \\\n",
    "        .withColumn(\"tokens\", split(trim(col(\"text_clean\")), \"\\\\s+\")) \\\n",
    "        .withColumn(\"doc_length\", size(col(\"tokens\")))\n",
    "    \n",
    "    # Explode tokens\n",
    "    word_doc_pairs = reviews_processed \\\n",
    "        .select(\"review_id\", \"doc_length\", explode(col(\"tokens\")).alias(\"word\")) \\\n",
    "        .filter(length(col(\"word\")) > 2)\n",
    "    \n",
    "    # Count term frequency per document\n",
    "    tf = word_doc_pairs \\\n",
    "        .groupBy(\"review_id\", \"doc_length\", \"word\") \\\n",
    "        .agg(count(\"*\").alias(\"term_count\")) \\\n",
    "        .withColumn(\"tf\", col(\"term_count\") / col(\"doc_length\"))\n",
    "    \n",
    "    print(\"\\nTerm Frequency (sample):\")\n",
    "    tf.orderBy(desc(\"tf\")).show(15)\n",
    "    \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 4: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "    \n",
    "    TF-IDF measures how important a word is to a document in a collection.\n",
    "    \n",
    "    Formula:\n",
    "    - TF = term frequency in document\n",
    "    - IDF = log(total documents / documents containing term)\n",
    "    - TF-IDF = TF * IDF\n",
    "    \n",
    "    High TF-IDF = word is common in this doc but rare across all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 4: TF-IDF Scoring\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Calculating TF-IDF scores:\")\n",
    "    \n",
    "    # Preprocess\n",
    "    reviews_processed = reviews_df \\\n",
    "        .withColumn(\"text_clean\", lower(regexp_replace(col(\"review_text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))) \\\n",
    "        .withColumn(\"tokens\", split(trim(col(\"text_clean\")), \"\\\\s+\")) \\\n",
    "        .withColumn(\"doc_length\", size(col(\"tokens\")))\n",
    "    \n",
    "    total_docs = reviews_processed.count()\n",
    "    \n",
    "    # Calculate TF\n",
    "    word_doc_pairs = reviews_processed \\\n",
    "        .select(\"review_id\", \"doc_length\", explode(col(\"tokens\")).alias(\"word\")) \\\n",
    "        .filter(length(col(\"word\")) > 2)\n",
    "    \n",
    "    tf = word_doc_pairs \\\n",
    "        .groupBy(\"review_id\", \"doc_length\", \"word\") \\\n",
    "        .agg(count(\"*\").alias(\"term_count\")) \\\n",
    "        .withColumn(\"tf\", col(\"term_count\") / col(\"doc_length\"))\n",
    "    \n",
    "    # Calculate IDF (document frequency)\n",
    "    df_counts = word_doc_pairs \\\n",
    "        .select(\"word\", \"review_id\") \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"word\") \\\n",
    "        .agg(count(\"*\").alias(\"doc_frequency\"))\n",
    "    \n",
    "    # Join TF with IDF\n",
    "    from pyspark.sql.functions import log\n",
    "    \n",
    "    tfidf = tf.join(df_counts, \"word\") \\\n",
    "        .withColumn(\"idf\", log(lit(total_docs) / col(\"doc_frequency\"))) \\\n",
    "        .withColumn(\"tfidf\", col(\"tf\") * col(\"idf\")) \\\n",
    "        .select(\"review_id\", \"word\", \"tf\", \"idf\", \"tfidf\")\n",
    "    \n",
    "    print(\"\\nTF-IDF Scores (sample - highest scores):\")\n",
    "    tfidf.orderBy(desc(\"tfidf\")).show(20)\n",
    "    \n",
    "    # Top terms per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Top 5 important terms per document (by TF-IDF):\")\n",
    "    window = Window.partitionBy(\"review_id\").orderBy(desc(\"tfidf\"))\n",
    "    \n",
    "    top_terms = tfidf \\\n",
    "        .withColumn(\"rank\", row_number().over(window)) \\\n",
    "        .filter(col(\"rank\") <= 5) \\\n",
    "        .groupBy(\"review_id\") \\\n",
    "        .agg(collect_list(\"word\").alias(\"top_terms\"))\n",
    "    \n",
    "    # Join with original reviews\n",
    "    reviews_with_terms = reviews_df.join(top_terms, \"review_id\")\n",
    "    reviews_with_terms.select(\"review_id\", \"review_text\", \"top_terms\").show(10, truncate=False)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 5: Building a Product Search Index\n",
    "\n",
    "    \n",
    "    Create a searchable index of products based on their attributes.\n",
    "    This enables full-text search over product names, categories, and brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 5: Product Search Index\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Building searchable product index:\")\n",
    "    \n",
    "    # Combine searchable fields\n",
    "    products_searchable = products_df.withColumn(\n",
    "        \"search_text\",\n",
    "        lower(concat_ws(\" \", col(\"name\"), col(\"category\"), col(\"brand\")))\n",
    "    )\n",
    "    \n",
    "    print(\"\\nProducts with search text:\")\n",
    "    products_searchable.select(\"product_id\", \"name\", \"search_text\").show(10, truncate=False)\n",
    "    \n",
    "    # Tokenize and create inverted index\n",
    "    products_tokenized = products_searchable \\\n",
    "        .withColumn(\"tokens\", split(col(\"search_text\"), \"\\\\s+\"))\n",
    "    \n",
    "    # Build inverted index\n",
    "    product_index = products_tokenized \\\n",
    "        .select(col(\"product_id\"), explode(col(\"tokens\")).alias(\"term\")) \\\n",
    "        .filter(length(col(\"term\")) > 2) \\\n",
    "        .groupBy(\"term\") \\\n",
    "        .agg(collect_list(\"product_id\").alias(\"product_ids\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Product search index (sample):\")\n",
    "    product_index.show(20, truncate=False)\n",
    "    \n",
    "    # Search function\n",
    "    def search_products(search_term):\n",
    "        \"\"\"Search for products containing the term\"\"\"\n",
    "        print(f\"\\nðŸ” Searching for: '{search_term}'\")\n",
    "        results = product_index.filter(col(\"term\") == search_term.lower())\n",
    "        \n",
    "        if results.count() > 0:\n",
    "            product_ids = results.first()[\"product_ids\"]\n",
    "            print(f\"Found {len(product_ids)} products\")\n",
    "            \n",
    "            # Get product details\n",
    "            matching_products = products_df.filter(col(\"product_id\").isin(product_ids))\n",
    "            matching_products.select(\"product_id\", \"name\", \"category\", \"brand\").show(10, truncate=False)\n",
    "        else:\n",
    "            print(\"No products found\")\n",
    "    \n",
    "    # Example searches\n",
    "    search_products(\"electronics\")\n",
    "    search_products(\"brandA\")\n",
    "    \n",
    "    return product_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 6: Product Recommendations (Collaborative Filtering - Simple)\n",
    "\n",
    "    \n",
    "    Build a simple recommendation system:\n",
    "    - Find products frequently bought together\n",
    "    - Recommend based on co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 6: Product Recommendations\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Finding products frequently bought together:\")\n",
    "    \n",
    "    # Get user purchase history\n",
    "    user_products = transactions_df \\\n",
    "        .filter(col(\"status\") == \"completed\") \\\n",
    "        .groupBy(\"user_id\") \\\n",
    "        .agg(collect_list(\"product_id\").alias(\"products\"))\n",
    "    \n",
    "    print(\"\\nUser purchase history (sample):\")\n",
    "    user_products.show(5, truncate=False)\n",
    "    \n",
    "    # Self-join to find product pairs bought by same users\n",
    "    from pyspark.sql.functions import array_intersect, array_except, array_union\n",
    "    \n",
    "    # Count co-occurrences\n",
    "    product_pairs = transactions_df \\\n",
    "        .filter(col(\"status\") == \"completed\") \\\n",
    "        .select(\"user_id\", \"product_id\") \\\n",
    "        .alias(\"t1\") \\\n",
    "        .join(\n",
    "            transactions_df.filter(col(\"status\") == \"completed\")\n",
    "            .select(\"user_id\", \"product_id\")\n",
    "            .alias(\"t2\"),\n",
    "            col(\"t1.user_id\") == col(\"t2.user_id\")\n",
    "        ) \\\n",
    "        .filter(col(\"t1.product_id\") < col(\"t2.product_id\")) \\\n",
    "        .groupBy(col(\"t1.product_id\").alias(\"product_a\"), col(\"t2.product_id\").alias(\"product_b\")) \\\n",
    "        .agg(count(\"*\").alias(\"co_occurrence_count\")) \\\n",
    "        .orderBy(desc(\"co_occurrence_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Products frequently bought together:\")\n",
    "    product_pairs.show(20)\n",
    "    \n",
    "    # Enrich with product names\n",
    "    product_pairs_enriched = product_pairs \\\n",
    "        .join(products_df.alias(\"pa\"), col(\"product_a\") == col(\"pa.product_id\")) \\\n",
    "        .join(products_df.alias(\"pb\"), col(\"product_b\") == col(\"pb.product_id\")) \\\n",
    "        .select(\n",
    "            col(\"product_a\"),\n",
    "            col(\"pa.name\").alias(\"product_a_name\"),\n",
    "            col(\"product_b\"),\n",
    "            col(\"pb.name\").alias(\"product_b_name\"),\n",
    "            col(\"co_occurrence_count\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"co_occurrence_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Product recommendations (with names):\")\n",
    "    product_pairs_enriched.show(15, truncate=False)\n",
    "    \n",
    "    # Given a product, recommend others\n",
    "    def recommend_for_product(product_id):\n",
    "        \"\"\"Recommend products based on co-purchases\"\"\"\n",
    "        print(f\"\\nðŸŽ¯ Recommendations for product {product_id}:\")\n",
    "        \n",
    "        recommendations = product_pairs_enriched.filter(\n",
    "            (col(\"product_a\") == product_id) | (col(\"product_b\") == product_id)\n",
    "        )\n",
    "        \n",
    "        recommendations.show(10, truncate=False)\n",
    "    \n",
    "    # Example\n",
    "    recommend_for_product(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSON 7: Text Similarity\n",
    "\n",
    "    \n",
    "    Find similar documents based on shared terms.\n",
    "    Useful for duplicate detection, clustering, and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LESSON 7: Text Similarity\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Finding similar reviews based on word overlap:\")\n",
    "    \n",
    "    # Preprocess and tokenize\n",
    "    reviews_processed = reviews_df \\\n",
    "        .withColumn(\"text_clean\", lower(regexp_replace(col(\"review_text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))) \\\n",
    "        .withColumn(\"tokens\", split(trim(col(\"text_clean\")), \"\\\\s+\")) \\\n",
    "        .select(\"review_id\", \"review_text\", \"tokens\")\n",
    "    \n",
    "    # Self-join to compare all pairs\n",
    "    similarity = reviews_processed.alias(\"r1\").join(\n",
    "        reviews_processed.alias(\"r2\"),\n",
    "        col(\"r1.review_id\") < col(\"r2.review_id\")\n",
    "    )\n",
    "    \n",
    "    # Calculate Jaccard similarity: |A âˆ© B| / |A âˆª B|\n",
    "    @udf(FloatType())\n",
    "    def jaccard_similarity(tokens1, tokens2):\n",
    "        \"\"\"Calculate Jaccard similarity between two token lists\"\"\"\n",
    "        if not tokens1 or not tokens2:\n",
    "            return 0.0\n",
    "        \n",
    "        set1 = set(tokens1)\n",
    "        set2 = set(tokens2)\n",
    "        \n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "        \n",
    "        return float(intersection) / float(union) if union > 0 else 0.0\n",
    "    \n",
    "    similarity_scores = similarity \\\n",
    "        .withColumn(\"similarity\", jaccard_similarity(col(\"r1.tokens\"), col(\"r2.tokens\"))) \\\n",
    "        .filter(col(\"similarity\") > 0.3) \\\n",
    "        .select(\n",
    "            col(\"r1.review_id\").alias(\"review_1\"),\n",
    "            col(\"r1.review_text\").alias(\"text_1\"),\n",
    "            col(\"r2.review_id\").alias(\"review_2\"),\n",
    "            col(\"r2.review_text\").alias(\"text_2\"),\n",
    "            col(\"similarity\")\n",
    "        ) \\\n",
    "        .orderBy(desc(\"similarity\"))\n",
    "    \n",
    "    print(\"\\nSimilar reviews (similarity > 0.3):\")\n",
    "    similarity_scores.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Notebook completed! Check the next notebook to continue learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run to stop Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_spark_session(spark)\n",
    "# print(\"âœ… Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
